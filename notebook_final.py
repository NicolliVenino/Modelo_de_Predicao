# -*- coding: utf-8 -*-
"""notebook_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HJMr9GHxAoxly_wVCKnxr3LRlAzdj9Hu

# Introdu√ß√£o

O presente notebook tem o fito de suprir a demanda do Governo do Distrito Federal, a qual foi exposta por interm√©dio de um Ideathon em 2026.O arquivo contempla o c√≥digo python utilizado para tal efeito.

## 1. Configura√ß√µes Iniciais

### 1.1 Instala√ß√£o de Depend√™ncias
"""

!pip install -q transformers accelerate torch scikit-learn pandas openpyxl

"""### 1.2 Importa√ß√£o das Bibliotecas Utilizadas"""

import pandas as pd
import numpy as np
import torch

from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.optim import AdamW

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import recall_score, precision_score
from sklearn.utils.class_weight import compute_class_weight

"""### 1.3 Defini√ß√£o do Modelo e Ajuste de Hiperpar√¢metros"""

MODEL_NAME = "neuralmind/bert-base-portuguese-cased"
MAX_LEN = 256
BATCH_SIZE = 8
EPOCHS = 3
LR = 2e-5
THRESHOLD = 0.45

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

"""### 1.4 Carregamento dos Dados"""

df = pd.read_csv("/content/AMOSTRA_e-SIC.csv")

df = df.rename(columns={
    "Texto Mascarado": "text",
    "label": "label"
})

df = df[["text", "label"]]
df.dropna(inplace=True)

df.head()

"""### 1.5 Customiza√ß√£o do Dataset

A customiza√ß√£o do dataset torna-se imprescind√≠vel para que o BERT funcione corretamente.
"""

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            padding="max_length",
            truncation=True,
            max_length=MAX_LEN,
            return_tensors="pt"
        )

        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

"""## 2. Desenvolvimento do Modelo Preditivo

### 2.1 Cria√ß√£o da Fun√ß√£o de Treino e de Avalia√ß√£o do Modelo
"""

def train_and_evaluate(train_texts, train_labels, val_texts, val_labels):

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    train_dataset = TextDataset(train_texts, train_labels, tokenizer)
    val_dataset = TextDataset(val_texts, val_labels, tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    class_weights = compute_class_weight(
        class_weight="balanced",
        classes=np.array([0, 1]),
        y=train_labels
    )

    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)

    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=2
    ).to(device)

    optimizer = AdamW(model.parameters(), lr=LR)
    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)

    # -------- TREINO --------
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0

        for batch in train_loader:
            optimizer.zero_grad()

            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            loss = loss_fn(outputs.logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f}")

    # -------- AVALIA√á√ÉO --------
    model.eval()
    probs = []
    y_true = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

            prob = torch.softmax(outputs.logits, dim=1)[:, 1]
            probs.extend(prob.cpu().numpy())
            y_true.extend(batch["labels"].numpy())

    return np.array(probs), np.array(y_true)

"""### 2.2 Valida√ß√£o Cruzada

A valida√ß√£o cruzada configura-se como uma t√©cnica para garantir que o modelo n√£o decorou os dados e consegue manter a performance mesmo frente a outros dados.
"""

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

recalls = []
precisions = []

fold = 1

for train_idx, val_idx in skf.split(df["text"], df["label"]):

    print(f"\n======================")
    print(f"üîÅ Fold {fold}")
    print("======================")

    train_texts = df.iloc[train_idx]["text"]
    train_labels = df.iloc[train_idx]["label"]

    val_texts = df.iloc[val_idx]["text"]
    val_labels = df.iloc[val_idx]["label"]

    probs, y_true = train_and_evaluate(
        train_texts, train_labels,
        val_texts, val_labels
    )

    y_pred = (probs >= THRESHOLD).astype(int)

    recall = recall_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)

    print(f"Recall: {recall:.3f}")
    print(f"Precision: {precision:.3f}")

    recalls.append(recall)
    precisions.append(precision)

    fold += 1

"""## 3. An√°lise das M√©tricas Finais

An√°lise final das m√©tricas de Recall e Precision.
"""

print("\nüìä Resultado Final, ap√≥s valida√ß√£o cruzada:")

print(f"Recall m√©dio: {np.mean(recalls):.3f}")
print(f"Desvio recall: {np.std(recalls):.3f}")

print(f"Precision m√©dia: {np.mean(precisions):.3f}")